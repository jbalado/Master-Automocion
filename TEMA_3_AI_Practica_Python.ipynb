{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica: Inteligencia Artificial en Python\n",
    "\n",
    "El objetivo de esta práctica es que el alumno se familiarice con la Inteligencia Artificial en un caso de estudio práctico. Para lo cual, continuando la practica anterior, se ha seleccionado un problema clásico en el procesado de nubes de puntos: la segmentación semántica de una nube. Con el mismo tipo de datos que en la Práctica 2, nubes de puntos adquiridas con Velodyne64 y disponibles en SemanticKITTI.\n",
    "\n",
    "Un problema de segmentación semántica consiste en clasificar cada elemento básico que componen los datos. En imagen, se trata de pixeles, en nubes de puntos, se trata de puntos. A veces, la segmentación semántica también se denomina clasificación pixel por pixel, o punto por punto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos\n",
    "\n",
    "En esta práctica vamos a ver fragmentos de código que expliquen cómo se preparar datos para un algoritmo de IA, entrenarlo y usarlo para realizar predicciones. Las operaciones que se estudiarán a continuación son:\n",
    "\n",
    "- Lectura y escritura de nubes de puntos como datos txt\n",
    "- Preparación de datos para Inteligencia Artificial\n",
    "- Extracción de características\n",
    "- Entrenamiento de algoritmos\n",
    "- Análisis de los resultados mediante métricas: matrices de confusión, precision, recall, f1-score y accuracy\n",
    "- Visualización de la clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1\n",
    "\n",
    "Los algoritmos empleados en inteligencia artificial son bastante complejos. Aunque podemos programar un algoritmo con conocimientos simples sobre el tema, conseguir que alcance las tasas de acierto de algoritmos ya desarrollados por otros autores requiere un fuerte conocimeinto en matemáticas y computación, además de pontentes servidores donde realizar pruebas. Por suerte, la mayoría de algoritmos de IA están en código abierto y recopilados en librerías.\n",
    "\n",
    "La primera tarea consiste en preparar el notebook para trabajar con él:\n",
    "- Carga en el direcorio las carpetas *Nubes* e *img*, \n",
    "- Instala la librería *pyntcloud*\n",
    "- Comprueba si la librería sklearn está instalada. **Nota**: sklearn aparece en el instalador como scikit-learn.\n",
    "\n",
    "Puedes encontrar la ayuda de esta librería en:\n",
    "- https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instalada la librería, vamos a importar todas las funciones que necesitamos. Si alguna da error, comprueba que está instalada en el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyntcloud import PyntCloud\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura y escritura de nubes de puntos como datos txt\n",
    "\n",
    "Los algoritmos basados en IA necesitan de una fase de entrenamiento antes de poder emplearlos para clasificar nuevos datos. Como dato de entrada vamos a emplear las nubes de puntos 000036 (train) y 000079 (test). Pero en este caso no vamos a cargarlas como pcd, si no como txt, puesto que este tipo de formato, junto con csv, es el más común en el que podemos encontrar otros tipos de datos para IA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datos\n",
    "train_data = np.loadtxt(\"Nubes/000036.txt\", delimiter=' ')\n",
    "#Visualización\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La organización de los datos en IA se distribuye en matrices donde cada fila se corresponde con una muestra y cada columna con un atributo. Esto es muy similar a como se distribuyen las nubes de puntos. En los datos anteriores podemos ver que cada punto contiene 3 coordeandas y la cuarta columna se corresponde con la etiqueta siguiendo el siguiente código:\n",
    "- 1: coche\n",
    "- 2: edificio\n",
    "- 3: suelo\n",
    "- 4: vegetación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de datos para Inteligencia Artificial\n",
    "\n",
    "Pero estos datos no pueden emplearse directamente en un algoritmo de IA, deben dividirse en una matriz de atributos NxM, siendo N el número de muestras y M el número de atributos, y una matriz de etiquetas (Nx1).\n",
    "\n",
    "Además, en nubes de puntos no se pueden emplear las coordenadas como atributos de entrenamiento para IA, puesto que la posición de los puntos en coordenadas absolutas no representa ni tiene relación con nuevos datos. En nubes de puntos, las coordenadas son empleadas para extraer nuevas características geométricas que sí son aptas para el clasificador.\n",
    "\n",
    "Por lo tanto, antes de nada, vamos a dividir la matriz de entrada en dos matrices:\n",
    "- Matriz de coordenadas (definida como un dataframe con título de columnas, necesario para su transformación a Pyntcloud object)\n",
    "- Matriz de etiquetas (definida como numpy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer matriz de coordenadas\n",
    "coord = pd.DataFrame(list(zip(train_data[:,0],train_data[:,1],train_data[:,2])))  \n",
    "\n",
    "# Asignar título a columnas\n",
    "coord.columns =['x', 'y', 'z']\n",
    "\n",
    "#Visualizar\n",
    "print(coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer matriz de etiquetas\n",
    "train_labels = train_data[:,3]\n",
    "\n",
    "#Visualizar\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de características\n",
    "\n",
    "Como se menciona anteriormente, las coordenadas no son útiles para el entrenamiento de un algoritmo basado en IA, y por lo tanto tenemos que extraer características geométricas. Aunque podemos calcularlas empleada directamente la matriz, es más fácil convertir la matriz a un objeto cloud en la librería pyntcloud y emplear sus funciones para calcularlas, como en la Práctica 2. \n",
    "\n",
    "Para una conversión correcta, cloud no admite un numpyarray, si no que tiene que emplearse un dataframe con los títulos \"x\", \"y\", y \"z\", calculados en el paso anterior.\n",
    "\n",
    "Las características empleadas para el entrenamiento serán:\n",
    "- Normales\n",
    "- Curvatura\n",
    "- Omnivariaza\n",
    "- Linealidad\n",
    "- Planaridad\n",
    "- Dispersion\n",
    "\n",
    "El uso de estas características está ampliamente extendido en nubes de puntos, su cálculo se encuentra explicado en el siguiente trabajo científico:\n",
    "- Weinmann, M., Jutzi, B., & Mallet, C. (2014). Semantic 3D scene interpretation: A framework combining optimal neighborhood size selection with relevant features. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2(3), 181."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion\n",
    "cloud = PyntCloud(coord)\n",
    "\n",
    "#Visualización\n",
    "print(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2\n",
    "\n",
    "Calcula las normales para 25 vecinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de 25 vecinos\n",
    "k_neighbors_25 = COMPLETAR\n",
    "\n",
    "# Cálculo de normales\n",
    "COMPLETAR\n",
    "\n",
    "#Visualización\n",
    "cloud.points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, calcularemos las características basadas en autovalores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálulo de autovalores\n",
    "eigenvalues = cloud.add_scalar_field(\"eigen_values\", k_neighbors=k_neighbors_25)\n",
    "\n",
    "# Cálculo de características\n",
    "cloud.add_scalar_field(\"curvature\", ev=eigenvalues)\n",
    "cloud.add_scalar_field(\"omnivariance\", ev=eigenvalues)\n",
    "cloud.add_scalar_field(\"linearity\", ev=eigenvalues)\n",
    "cloud.add_scalar_field(\"planarity\", ev=eigenvalues)\n",
    "cloud.add_scalar_field(\"sphericity\", ev=eigenvalues)\n",
    "\n",
    "# Visualización de los datos\n",
    "cloud.points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ser usadas en el algoritmo, las características deben ser devueltas al formato numpy array. Además, no todas las características de \"points\" son útiles, seleccionaremos las normales y las calculadas de los autovalores, pero no las coordenadas ni los autovalores en sí mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de características\n",
    "train_features = cloud.points[['nx(26)','ny(26)','nz(26)','curvature(26)','omnivariance(26)','linearity(26)','planarity(26)','sphericity(26)',]].to_numpy()\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones\n",
    "\n",
    "Antes de proseguir, vamos a definir unas funciones\n",
    "\n",
    "Hemos visto paso por paso como leer los datos de entrada, dividirlos y extraer sus características. Estas características son las empleadas en el entrenamiento, pero también son necesarias para la clasificación futura, por lo tanto, es necesario para cada muestra extraer estas características y generar una matriz de datos cuyos atributos se correspondan con el orden de entrenamiento. Para no repetir código cada vez que carguemos una matriz de datos, definiremos dos funciones. La primera separará los datos. La segunda, extraerá las características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función servirá para separar los datos de entrada en coordenadas y etiquetas\n",
    "def separar_input(input_matrix):\n",
    "    coord = pd.DataFrame(list(zip(input_matrix[:,0],input_matrix[:,1],input_matrix[:,2])))  \n",
    "    coord.columns =['x', 'y', 'z']\n",
    "    labels = input_matrix[:,3]\n",
    "    return coord, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 3\n",
    "\n",
    "Genera una función para la extracción de características que integre los pasos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_features(coord):\n",
    "    # Crear nube\n",
    "    cloud = COMPLETAR\n",
    "    # Calcular vecinos\n",
    "    k_neighbors_25 = COMPLETAR\n",
    "    # Calcular y añadir normales\n",
    "    cloud.add_scalar_field(\"normals\", k_neighbors=k_neighbors_25)\n",
    "    # Calcular y añadir eigenvalues\n",
    "    eigenvalues = COMPLETAR\n",
    "    # Calcular y añadir otras caracteristicas geometricas\n",
    "    cloud.add_scalar_field(\"curvature\", ev=eigenvalues)\n",
    "    COMPLETAR\n",
    "    COMPLETAR\n",
    "    COMPLETAR\n",
    "    COMPLETAR\n",
    "    # Transformar dataframe de puntos a nparray de features (por orden)\n",
    "    features = COMPLETAR\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si has sido capaz de copiar y pegar correctamente el código anterior para generar la función, el nuevo código para dividir y extraer características queda resumido en dos lineas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar datos de entrada\n",
    "train_coord,train_labels = separar_input(train_data)\n",
    "\n",
    "# Extraer características\n",
    "train_features = extraer_features(train_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de algoritmos\n",
    "\n",
    "En este paso llegamos al núcleo del algoritmo. Una vez todos los datos están preparados procedemos al entrenamiento. En esta práctica vamos a entrenar y usar los dos algoritmos más empleados en la actualidad: \n",
    "- Support Vector Machine. Dado un conjunto de puntos, en el que cada uno de ellos pertenece a una de dos posibles categorías, un algoritmo basado en SVM construye un modelo capaz de predecir si un punto nuevo (cuya categoría desconocemos) pertenece a una categoría o a la otra.\n",
    "\n",
    "<center> <img src=\"img/svm.png\"></center>\n",
    "<center>Fuente: https://en.wikipedia.org/wiki/Support-vector_machine</center>\n",
    "\n",
    "- Random Forest. Es una combinación de árboles predictores tal que cada árbol depende de los valores de un vector aleatorio probado independientemente y con la misma distribución para cada uno de estos. El resultado de la calsificación es la clase predicha mayoritariamente por todos los árboles.\n",
    "\n",
    "<center> <img src=\"img/rf.png\"></center>\n",
    "<center>Fuente: https://es.wikipedia.org/wiki/Random_forest</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez realizado todo el trabajo, solo hay que darle los datos al algoritmo para que entrene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir clasificador SVM\n",
    "clf_svm = svm.SVC()\n",
    "\n",
    "#Entrenar (esto puede tardar varios minutos)\n",
    "clf_svm.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir clasificador RF\n",
    "clf_rf = RandomForestClassifier()\n",
    "\n",
    "#Entrenar (esto debería ir más rápido que el anterior)\n",
    "clf_rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de los resultados mediante métricas\n",
    "\n",
    "Las métricas nos permiten saber cuán bueno es nuestro algoritmo entrenado respecto a la realidad. Si las métricas se aplican sobre los datos de entrenamiento sabremos si nuestro algoritmo ha sido capaz de aprender de los datos proporcionados. Pero para obtener resultados fiables y contrastables sobre el buen funcionamiento del algoritmo tendremos que testearlo sobre datos distintos de los usados para entrenar. Por defecto, la librería sklearn ofrece funciones para emplear las métricas más comunes:\n",
    "\n",
    "\n",
    "<center> <img src=\"img/matcon.jpg\"></center>\n",
    "<center>Fuente: Confusion Matrix - Applied Deep Learning with Keras</center>\n",
    "\n",
    "\n",
    "<center> <img src=\"img/met.png\"></center>\n",
    "<center>Fuente: https://www.researchgate.net/</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, apliquemos las metricas a los datos de entrenamiento para observar qué algoritmo ha aprendido mejor, es decir, identificado y extraido más información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clasificamos los puntos (a través de las features) (Esto puede tardar unos minutos)\n",
    "\n",
    "# Con el SVM\n",
    "train_predictions_SVM = clf_svm.predict(train_features)\n",
    "\n",
    "# Con el Random Forest\n",
    "train_predictions_RF = clf_rf.predict(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de métricas para el algoritmo SVM\n",
    "\n",
    "# Aquí definimos los datos de referencia (ground truth)\n",
    "y_test = train_labels\n",
    "\n",
    "# Aquí definimos los datos que hemos calculado\n",
    "y_pred = train_predictions_SVM\n",
    "\n",
    "# Calcula y muestra la matriz de confusión\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "# Calcula y muestra estadísticas por clase\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# Calcula y muestra estadísticas globales\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 4\n",
    "\n",
    "Analiza los resultados. ¿Cuál es la accuracy? ¿Dónde se produce la mayor confusión? ¿Qué clase se clasifica mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 5\n",
    "\n",
    "Calcula las métricas para el conjunto de entrenamiento con el Random Forest entrenado. ¿El resultado es mejor? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de métricas para el algoritmo SVM\n",
    "\n",
    "# Aquí definimos los datos de referencia (ground truth)\n",
    "y_test = COMPLETAR\n",
    "\n",
    "# Aquí definimos los datos que hemos calculado\n",
    "y_pred = COMPLETAR\n",
    "\n",
    "# Calcula y muestra la matriz de confusión\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "# Calcula y muestra estadísticas por clase\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# Calcula y muestra estadísticas globales\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 6\n",
    "\n",
    "Calcula las métricas para el conjunto de testeo con los dos algoritmos. ¿Cuál fue mejor? ¿Observas underfitting u overfitting en algún algoritmo? ¿Cómo se puede solucionar? **Nota:** Carga la nube de testeo 000079.txt, separa los datos, extrae las features, y clasificalas. Usa las funciones creadas anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datos\n",
    "test_data = COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar datos de entrada\n",
    "test_coord,test_labels = COMPLETAR\n",
    "\n",
    "# Extraer características\n",
    "test_features = COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clasificamos los puntos (a través de las features) (Esto puede tardar unos minutos)\n",
    "\n",
    "# Con el SVM\n",
    "test_predictions_SVM = COMPLETAR\n",
    "\n",
    "# Con el Random Forest\n",
    "test_predictions_RF = COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de métricas para el algoritmo SVM\n",
    "\n",
    "# Aquí definimos los datos de referencia (ground truth)\n",
    "y_test = COMPLETAR\n",
    "\n",
    "# Aquí definimos los datos que hemos calculado\n",
    "y_pred = COMPLETAR\n",
    "\n",
    "# Calcula y muestra la matriz de confusión\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "# Calcula y muestra estadísticas por clase\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# Calcula y muestra estadísticas globales\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de métricas para el algoritmo RF\n",
    "\n",
    "# Aquí definimos los datos de referencia (ground truth)\n",
    "y_test = COMPLETAR\n",
    "\n",
    "# Aquí definimos los datos que hemos calculado\n",
    "y_pred = COMPLETAR\n",
    "\n",
    "# Calcula y muestra la matriz de confusión\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "# Calcula y muestra estadísticas por clase\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# Calcula y muestra estadísticas globales\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de la clasificación\n",
    "\n",
    "Una ventaja de las nubes de puntos es que podemos visualizar los resultados, puesto que son datos geométricos. Si usamos la inteligencia artificial para resolver problemas exclusivamente numéricos, no es posible una visualización de los datos tan claro. La visualización de los datos es muy relevante para poder identificar posibles fallos que pasan desapercibidos en las métricas.\n",
    "\n",
    "Por último, procedemos a exportar los resultados en una nube para poder visualizarla en CloudCompare. La nube exportada tendrá las siguientes columnas: \n",
    "- 3 columnas de coordenadas\n",
    "- 1 columna de labels (ground truth)\n",
    "- 1 columna de predicción SVM\n",
    "- 1 columna de predicción RF\n",
    "\n",
    "¿Se detecta algún problema que no se detectaba en las métricas? ¿Son los resultados tan parecidos como sugieren las métricas? ¿Qué clases se han clasificado mejor? Pudiendo visualizar los problemas, ¿se te ocurre alguna solución mejor que la propuesta solo con las métricas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar\n",
    "# Definicion de la ruta y nombre del archivo\n",
    "ruta = \"Nubes/00000079_predicted.txt\"\n",
    "\n",
    "#Seleccion de datos \n",
    "datos = np.column_stack((test_data,test_predictions_SVM,test_predictions_RF))\n",
    "\n",
    "# Guardado\n",
    "np.savetxt(ruta,datos,delimiter=' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
